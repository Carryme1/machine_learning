# 感知机
- 这是SVM算法的基础。感知机是二分裂的线性模型，其输入为实例的特征向量，输出为实例的列别，取 **+1和-1**。注意这里，输出一定是 **+1 -1**。

- 由输入空间到输出空间的如下函数  
```math
f(x)=sign(w\cdot x+b)
```
称为感知机。其中，`w·x`表示`w`和`x`的内积。  
可以这样理解，线性方程
```math
w\cdot x+b=0
```
对应于特征空间**R**中的一个超平面*S*，其中`w`是超平面的法向量，`b`是超平面的截距。  

- 所以感知机学习的目标就是求得一个能够将训练集正实例点和负实例点完全正确区分的超平面。为了找到这样的超平面，即确定感知机模型参数`w,b`，需要使用损失函数将损失函数极小化。  
这里先补充点[梯度下降法](https://blog.csdn.net/walilk/article/details/50978864)的知识。强调一点，更新时是
```math
x_0 := x_0-\alpha * \frac{\partial f}{\partial x_0}

x_1:=x_1-\alpha * \frac{\partial f}{\partial x_1}

...

x_n:=x_n-\alpha * \frac{\partial f}{\partial x_n}
```
这里的`α`表示步长，而且是**相减**。  
点线距的公式是
```math
d=\frac{|Ax_0+By_0+C|}{||A^2+B^2||}
```

回到主题，任意一点`x0`到超平面S的距离是
```math
\frac{1}{||w||}*|w\cdot x_0 +b|
```
其次，对于错误分类的数据`(xi,yi)`来说
```math
-y_i*(w\cdot x_i+b)>0
```
成立。所以所有错误分类的点到超平面S的总距离为
```math
-\frac{1}{||w||}\sum_ {x_i\in M} y_i(w\cdot x+b)
```
不考虑`1/||w||`，则感知机学习的损失函数是
```math
L(w,b)=-\sum_ {x_i\in M}y_i(w\cdot x+b)
```
利用梯度下降法，求其最优解。`L(w,b)`对`wi,bi`求导，得
```math
\frac{\partial L}{\partial w_i}=-y_ix_i

\frac{\partial L}{\partial b_i}=-y_i
```
随机选取一个错误分类点`(xi,yi)`，对w,b进行跟新
```math
w_{i+1}=w_i+\eta y_ix_i

b_{i+1}=b_i+\eta y_i
```
η是步长（0< η <=1)。

- 所以感知学习算法的原始形式是
    1. 选取初值w0,b0，一般让其都为0
    2. 在训练集中选取数据(wi,yi)，随机选取
    3. 如果`yi*(w·xi + b)<=0`，则对w,b进行更新，因为此时说明是错误点
    4. 转至2，直至训练集中没有错误分类点

## 感知学习算法的对偶形式  
对错误分类点(xi,yi)通过
```math
w:=w+\eta y_ix_i

b:=b+\eta y_i
```
来逐步修改w,b，设修改n次，则w,b关于(xi,yi)的增量分别为
```math
\alpha _iy_ix_i

\alpha _iy_i
```
这里
```math
\alpha _i =n_i \eta
```
所以，最后学习到的w,b可以分别表示为(w,b初始化为0)
```math
w=\sum_{i=1}^{N}\alpha _iy_ix_i

b=\sum_{i=1}^{N}\alpha _iy_i
```
当`η=1`时，表示第i个实例点由于错分而进行更新的次数，换而言之，这样的实例对学习结果影响最好，**有点类似于SVM中的边界点**。

- 对偶形式的算法
1. `α←0`,`b←0`
2. 在训练集中选取(xi,yi)
3. 如果
```math
y_i(\sum_{j=1}^{N}\alpha _jy_jx_j\cdot x_i+b)\leq 0
```
则
```math
\alpha _i \leftarrow a_i+\eta

b \leftarrow b+ \eta y_i
```
4. 转至2知道没有错误分类的数据

对偶形式中训练实例仅以内机的形式出现，所以可以先将训练集中的实例间的内机计算出来并以矩阵的形式存储，这个矩阵就是Gram矩阵
```math
G=[x_i\cdot x_i]_{N\times N}
```

> 这里说明一下`ai`的求法。因为ni是整数，每次迭代后的结果应该是+1，所以a_i+1 = (ni+1)*η=ni * η + η= ai+η


---


