# 决策树
- 决策树学习包换包括3个步骤：特征选择、决策树生成和决策树修剪。  
- 决策树学习常用的算法有ID3、C4.5与CART
---
## 特征选择
常用的特征选择有3种，信息增益、增益率和基尼指数，分别对应ID3，C4.5和CART。
### 信息增益
- 首先提及一个概念，信息熵（也叫香农熵）。假定当前样本集合D中第K类样本所占的比例为pk，则D的信息熵为
```math
Ent(D)=-\sum_{k=1}^{|y|}p_k log_2 {p_k}
```
其中，|y|表示类标签的种类数，例如“好瓜、坏瓜”，|y|=2。信息熵表示数据的不确定度，信息熵越大，不确定度越大。
- 另外介绍一下信息增益
```math
Gain(D,a)=Ent(D)-\sum_{v=1}^{V} \frac{|D^v|}{|D|}Ent(D^v)
```
其中D为样例数，a为特征，v为特征a下的种类数。信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。
- 选择信息增益最大的作为划分属性
- 以信息增量为特征选择的算法是ID3算法，其对可取数目较多的属性有所偏好。

### 增益率
- 由于ID3对可取数目较多的属性有所偏好，为了改善这个偏好，而使用增益率，增益率对可取数目较少的属性有所偏好
```math
Gain\_ratio(D,a) =\frac{Gain(D,a)}{IV(a)}

IV(a)=-\sum_{v=1}^{V} \frac{|D^v|}{|D|}log_2{\frac{|D^v|}{|D|}}
```
- 选择增益率最高的作为划分属性
- 以信息率为特征选择的算法是C4.5算法

### 基尼指数
- CART树是使用基尼指数来进行选择划分属性，且CART是**二叉树**  
其中，样本集合D的基尼指数为
```math
Gini(D) = 1-\sum_{k=1}^{K}\left(\frac{|C_k|}{|D|} \right)^2
```
K属于类数。而特征A条件下集合D的基尼指数：
```math
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
```
- 选择是基尼指数最小的属性为划分属性

## 决策树生成
通常使用信息增益最大，信息率最大或基尼指数最小作为特征选择的准则，用递归的方法实现决策树。  
既然使用递归，就要有停止条件：
1. D中所有实例属于同一类`C_K`
2. 特征集为空集

**这里总结一下三种常用的决策树算法 ID3 C4.5 CART**
- ID3的输入X要求是离散的，标签Y也是离散，其对可取数目较多的属性有所偏好。使用信息增益作为特征选择，对连续属性没有办法处理。
- C4.5的输入X可以离散也可以连续，标签Y只能是离散的。如果X是离散的话，使用信息率来做特征选择；如果X是连续的话，可以对连续值进行离散化。首先对连续值进行升序排列，然后在每两个值之前取一个点，将其划分成两部分，然后使用增益率选择最佳划分点。  
其次，C4.5也可以对缺失值进行处理。为每个特征的取值赋予一个权重。
C4.5虽然各个方面都由于ID3，但其时间开销大。
- CART树是分类和回归树。可以用于分类，也可以用于回归。输入X和标签Y都可以离散或者连续。当输入X是离散、标签Y是离散的时候，使用基尼指数作为特征选择，与ID3和C4.5类似；当输入X连续、标签Y连续的时候，此时又可以分为回归树和模型树。  
**回归树**是返回一个单值。循环所有特征的所有特征值，寻找到一个切分点，将样本切分成两份，将这两个切分的样本分别计算平方误差，然后两个平均误差相加，即寻找一个特征值使集合的平方误差最小（如图所示），把该切分点对应的属性作为切分属性，分切点对应切分值，做一个二分法。返回均值u1作为叶节点。图中为了方便表示，是排序好的，但实际中，无需排序。另外，平方误差可以通过``样本的方差*样本个数``来求。  
![image](https://note.youdao.com/yws/api/personal/file/FCB4E99E783F45EFA5E2ED20BC809994?method=download&shareKey=f5e0aaf44761ab6ad895e780fa52287c)
**模型树**是返回一个线性分段函数。循环所有所有特征的所有特征值，寻找到一个切分点，使用拟合函数，例如线性拟合，使预测值和真实值的平方误差最小，类似于回归树，把该切分点对应的属性作为切分属性，分切点对应切分值，做一个二分法。返回一个线性分段函数作为也节点。  
当CART用于回归任务时，且其输入为连续值，包括特征和结果值。首先对不重复的特征值进行升序排序，然后循环这些特征值，以此特征值为切分点，计算对应结果的平方误差。当循环完所有特征值时，输出最小平方误差对应的特征和特征值。输出的结果是此特征的特征值为切分点，切分子集或者子集的均值。（即连续特征的离散化处理）  

特征X | 标签Y | ——
---|--- |---
离散 | 离散 |可以使用信息增益，增益率和基尼指数
离散 | 连续 |暂时不知道，但我猜测可以做二分法，有点类似CART的回归树
连续 | 离散 |可以使用C4.5，将连续值离散化
连续 | 连续 |可以使用CART

[**可以参考这篇文章**](https://www.cnblogs.com/wenyi1992/p/7685131.html)

- 决策树都是贪心算法，寻找的是局部最优值。
- 另外补充一下，二分法能使分类对特征值不那么敏感，是好的。当采用二分法的时候，可以不需要删除特征，即使特征使用过，后续也可以继续使用此特征。当不采用二分法的时候，判断完该属性，需要把该属性删除。**我的理解是这样，但不一定正确。**


## 剪枝
暂时理解不了剪枝的数学含义  
[**附上一篇能很好解释剪枝的文章**](https://blog.csdn.net/u014688145/article/details/53326910)
