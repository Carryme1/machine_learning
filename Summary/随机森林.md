备注：主要参考了[这篇博主的文章](http://www.cnblogs.com/maybe2030/p/4585705.html)，同时也推荐一个这个博主，写的内容挺详细的。

首先，随机森林的优点有：（来自[这篇博主](https://blog.csdn.net/qq547276542/article/details/78304454))
1. 具有极高的准确率
2. 随机性的引入，使得随机森林不容易过拟合
3. 随机性的引入，使得随机森林有很好的抗噪声能力
4. 能处理很高维度的数据，并且不用做特征选择
5. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化
6. 训练速度快，可以得到变量重要性排序
7. 容易实现并行化


随机森林主要有2个随机：有放回的随机抽样，和对特征的随机抽样。  
1. 如果训练集的大小是N，则有放回的随机抽取M个样本，重复X次，则会生成X个具有M个样本的不同训练集（这种采样方式称为bootstrap sample方法）。
> 为什么要随机抽样训练集？  
>　　如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；  
>
>为什么要有放回地抽样？   
>　　我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

2. 假设每个样本的特征维度是M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
3. 每棵树都尽最大程度的生长，并且没有剪枝过程。    

一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，**使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）**。
    
    
    
　　上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。  
　　随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。  
　　我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。  
　　而这样的采样特点就允许我们进行oob估计，它的计算方式如下：  
　　（note：以样本为单位）  
　　1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；  
　　2）然后以简单多数投票作为该样本的分类结果；  
　　3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。  
oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。
