> **具体介绍可以见PDF**

# 总结
1. 回归与分类的区别，前者预测连续型变量，后者预测离散型变量；回归中求最佳系数的方法常用的是最小化误差的平方和；如果xTx可逆，那么回归算法可以使用；可以通过预测值和原始值的相关系数来度量回归方程的好坏
2. 当特征数大于样本总数时，为解决xTx不可逆的问题，我们可以通过引入岭回归来保证能够求得回归系数
3. 另外一种缩减算法是，前向逐步回归算法，它是一种贪心算法，每一步通过修改某一维度特征方法来减小预测误差，最后通过多次迭代的方法找到最小误差对应的模型
4. 缩减法可以看做是对一个模型增加偏差的同时减少方差，通过偏差方差折中的方法，可以帮助我们理解模型并进行改进，从而得到更好的预测结果  
[上面的总结来自这里](http://www.cnblogs.com/zy230530/p/6942458.html)

[这里介绍下标准化和归一化的不同](https://blog.csdn.net/pipisorry/article/details/52247379)，   [这篇也是](https://blog.csdn.net/u012101561/article/details/72506273)


