降维是为了防止“维数泛滥”，防止维度过多而影响影响速度；其次，通过有效的降维，可以去除噪声和不重要的信息。
# PCA算法
PCA算法（主成分分析），可以从两个方向分析**最小化投影误差**和**最大化方差**。从最大化方差上分析，本质上是找到一些投影方法，使得数据在这些投影方向上的方差最大，而且这些投影方向是正交的。  
为什么要是方差最大呢？因为方差越大，则样本在该特征上的差异越大，因此该特征越大。降维是提取出重要的特征，所以应该是方差最大。  
[具体的推导可以看这篇文章](http://www.cnblogs.com/lzllovesyl/p/5235137.html)  
> 还有种证明方法，见西瓜书P230

PCA算法的一般步骤是这样的：先对原始数据零均值化，然后求协方差矩阵，接着对协方差矩阵求特征向量和特征值，这些特征向量组成了新的特征空间。

另外，如何判断降到多少维度才是最好的呢？[同样也是见这里](http://www.cnblogs.com/lzllovesyl/p/5235137.html)  
```math
\frac{\sum_{i=1}^{k}v_{i}}{\sum_{i=1}^{n}v_{i}}
```

# SVD算法
[这里有一篇很棒的文章介绍](https://www.cnblogs.com/pinard/p/6251584.html)，下面的介绍的，大部分是来源于这篇文章。  
SVD是奇异值分解，不仅能运用在降维算法，还可以用于推荐系统，以及自然语言处理等领域。

SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×nm×n的矩阵，那么我们定义矩阵A的SVD为：
```math
A=U\sum V^T 
```
其中U是一个m×m的矩阵，Σ是一个m×n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个n×n的矩阵。U和V都是酉矩阵，即满足UTU=I,VTV=IUTU=I,VTV=I。下图可以很形象的看出上面SVD的定义：
![](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105115457425-1545975626.png)  
![](https://note.youdao.com/yws/api/personal/file/A4C45924F5B1493E9B7FF48D18D4F88B?method=download&shareKey=3ce5eeaad8738dbbfeca8db9e82abb07)  
这里说明一下，奇异值是AT*A矩阵的特征值的平方根（开方）。  
![](https://note.youdao.com/yws/api/personal/file/A67D8C556C2240BE9DD2D2154DE229B9?method=download&shareKey=3d79c8835eb0bee40e3285d07e78cf96)  
**所以，一个很大的矩阵，可以用3个小矩阵来保存，大大节省节省了空间。**  
![](https://note.youdao.com/yws/api/personal/file/577C9CD4287F492CA3288D48BE41BCDB?method=download&shareKey=30511d7f57f46f82e24fb539c2a81344)  
**注意一下最后一段，可以左奇异矩阵对行进行压缩，利用右奇异矩阵对列进行压缩（即PCA降维），这就是SVD和PCA的联系。**
