[书上的代码](https://blog.csdn.net/sinat_17196995/article/details/70332664)  

# 聚类
接下来转到无监督学习。无监督学习中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律。**聚类与分类的最大不同在于**，分类的目标事先已知，而聚类则不一样。因为其产生的结果与分类相同，而只是类别没有预先定义。  
聚类分析试图将相似对象归入同一簇，将不相似对象归到不同簇。  

## K-Mean聚类
首先随机确定k个初始点作为质心，然后将数据集中到每个点分配到一个簇中，具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。直至所有质心不变化或变化程度小为止。  

那么如何判断聚类的效果呢？一种用于度量聚类效果的指标是**SSE（误差平方和）**，即每个点到其质心的距离的平方。SSE值越小表示数据点越接近它们的质心，聚类效果越好。因为对误差采用了平方，因此更加重视那些远离质心的点。  
一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下，提高簇的质量。  
所以，有两种方法可以提高簇的质量，即降低SSE：
1. 合并：将**每个**样本点都当成一个簇，然后找出距离最近的两个簇进行合并，直至达到k值。关键是如何计算聚类簇之间的距离。对应的算法是**AGNES**。
2. 分裂：将**所有**样本点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE值。直至达到k值。对应的算法是**二分K-均值**。

### 二分K-Mean算法
将**所有**样本点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE值。直至达到k值。  
另外，另一种选择簇的方法是选择SSE最大的簇进行划分。

### AGNES算法
将**每个**样本点都当成一个簇，然后找出距离最近的两个簇进行合并，直至达到k值。关键是如何计算聚类簇之间的距离。  
对应的距离可以这样计算
![](https://note.youdao.com/yws/api/personal/file/71682DADAF814047A8AAD003B62D1CC7?method=download&shareKey=e83ab1604a0df9221df422b212db8822)  

具体的算法是
![](https://note.youdao.com/yws/api/personal/file/9CF5E0BC89F64DEB9AF4A48D49E893F2?method=download&shareKey=abd88cb8c80e0a7402d572941464ea09)

# 小结 
聚类是一种无监督聚类算法，无监督指的是事先不知道所需要查找的内容（无目标变量）。聚类将数据点归入多个簇中，相似的数据点归入到同一个簇。有很多不同的方法来计算相似性。广泛使用的是K-均值算法：通过指定k值，随机分配k个质心，然后计算每个数据点到各个质心的距离，将点分配到距离最近的质心，重新计算每个簇的均值更新质心，反复迭代直到质心不在变化。（算法有效但初始k值不容易确定）   
另一种是二分K-均值算法：首先将所有点作为一个簇，然后采用k=2的K-均值算法进行划分，下一次迭代时选择两个簇中SSE（平方误差）最大的簇进行再次划分，直到簇数目达到给定的k值。二分K-均值的算法要优于K-均值算法，不容易收敛到局部最小。


---

结合上面的内容，聚类学习有两个基本问题——**性能度量和距离计算**。
1. 对于性能度量，我们需要通过某种性能度量来评估其好坏。也就是上面提及到的SSE，但不只是这一种评判标准。还有外部指标和内部指标。这里都涉及到这样一个概念：同一簇的样本尽可能彼此相似，不同簇的样本尽可能不同，即“簇内相似度”高而“簇间相似度”低。
2. 对于距离度量，对应上面的距离。也可以采用L1范数。我们常将属性划分为“连续属性”和“离散属性”，然而在讨论距离计算时，属性上是否定义了“序”关系更为重要。  

> 例如定义域为{1,2,3}，能直接再属性值上计算距离：“1”与“2”比较接近、与“3”比较远。这样的属性称为**有序属性**。而定义域为{飞机，火车，轮船}这样的属性不能直接计算距离，称为**无序属性**。对于无序属性，可以采用VDM算法来计算其距离，详情可见《机器学习》P200。当样本空间不同属性的重要性不同时，可以使用“加权距离”，即给每个属性赋予一个权重。

---
介绍另外一种聚类算法——学习向量化(Learning Vector Quntization, LVQ)。与K-Mean不同的是，LVQ假设数据样本带有类别标记，学习过程中利用样本的这些监督信息来辅助聚类。
![LVQ算法](https://note.youdao.com/yws/api/personal/file/4A6AFA6C1B464521A9C52B1020845D3D?method=download&shareKey=ab1fa67554395335a8899bf270fd9161)  

这里有篇文章[介绍了如何更新原型变量，即质心](https://blog.csdn.net/only2cyq/article/details/69174116)